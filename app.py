import streamlit as st
import pandas as pd
import os
import re
import io

# --- 1. é…ç½®èˆ‡æ ¸å¿ƒé‚è¼¯ ---

def standardize_unit(val, mapping):
    if pd.isna(val) or not isinstance(val, str):
        return val
    val = "".join(val.split()).upper()
    
    # æ­£å‰‡åŒ¹é…ï¼šå‰5ç¢¼è‹±æ•¸ + å¾Œé¢æœ‰å…§å®¹
    if re.match(r"^[A-Z0-9]{5}.+", val):
        return val
    
    # ç›´æ¥åœ¨å­—å…¸ä¸­
    if val in mapping:
        target = mapping[val]
        if re.match(r"^[A-Z0-9]{5}$", val): 
            return f"{val}{target}"
        else:
            return f"{target}{val}"
    
    # é€²éšæŸ¥æ‰¾ (æå–5ç¢¼ä»£ç¢¼)
    found_code = re.search(r"[A-Z0-9]{5}", val)
    if found_code:
        code = found_code.group()
        if code in mapping:
            return f"{code}{mapping[code]}"
    return val

# --- 2. è®€å–å°ç…§è¡¨ (é—œéµä¿®æ”¹ï¼šä¿ç•™åŸå§‹æ¸…å–®èˆ‡é †åº) ---

@st.cache_data
def get_full_reference():
    try:
        # å»ºè­°ç¶­æŒè®€å–æ•´å¼µè¡¨ï¼Œç”±é‚è¼¯ä¾†éæ¿¾
        ref_raw = pd.read_excel(REF_PATH, skiprows=0) 
        
        ref_list = []
        mapping = {}
        
        for _, row in ref_raw.iterrows():
            code = str(row['ä»£ç¢¼']).strip().upper() if pd.notna(row['ä»£ç¢¼']) else ""
            name = str(row['å–®ä½åç¨±']).strip() if pd.notna(row['å–®ä½åç¨±']) else ""
            
            # éæ¿¾æ‰ç„¡æ„ç¾©çš„è¡Œ
            if code in ["å–®ä½åç¨±", "NAN", "", "ä»£ç¢¼"] and name in ["å–®ä½åç¨±", "NAN", "", "ä»£ç¢¼"]:
                continue
            
            # --- æ ¸å¿ƒé‚è¼¯ä¿®æ­£ ---
            # åªæœ‰ç•¶ä»£ç¢¼é•·åº¦å‰›å¥½æ˜¯ 5 ç¢¼æ™‚ï¼Œæ‰åˆ¤å®šç‚ºé€šè¨Šè™•
            # å¦‚æœ code å…¶å¯¦æ˜¯å¾ˆé•·çš„ä¸€ä¸²å­—ï¼ˆå¦‚å€éƒ¨åç¨±ï¼‰ï¼Œæˆ–æ˜¯ name æ˜¯ç©ºçš„ï¼Œå°±é€²å…¥ else (æ¨™é¡Œæ¨¡å¼)
            if len(code) == 5 and code != "NAN":
                # é€™æ˜¯çœŸæ­£çš„é€šè¨Šè™•
                clean_name = name.replace("é€šè¨Šè™•", "").replace("é€šè¨Š", "")
                full_display = f"{code}{clean_name}"
                mapping[code] = clean_name
                mapping[clean_name] = code
                ref_list.append({"åŸå§‹æ¸…å–®": full_display, "is_unit": True})
            else:
                # é€²å…¥é€™è£¡ä»£è¡¨ï¼šcode æ˜¯ç©ºçš„ï¼Œæˆ–è€…æ˜¯é•·ä¸²çš„æ¨™é¡Œæ–‡å­—
                # æˆ‘å€‘å„ªå…ˆå– nameï¼Œå¦‚æœ name æ˜¯ç©ºçš„ï¼Œå°±å– code (å› ç‚ºæ¨™é¡Œå¯èƒ½è·‘å»ä»£ç¢¼æ¬„)
                title_text = name if name not in ["", "NAN"] else code
                
                if title_text not in ["", "NAN"]:
                    short_name = title_text[:4] # åªå–å‰å››å€‹å­—
                    ref_list.append({"åŸå§‹æ¸…å–®": short_name, "is_unit": False})
            
        to_csv_path = 'C:\\Users\\user\\workplace\\RFA\\ref_df.csv'
        pd.DataFrame(ref_list).to_csv(to_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… æå–çš„è³‡æ–™å·²æˆåŠŸå„²å­˜è‡³ '{to_csv_path}'ã€‚")
        
        return pd.DataFrame(ref_list), mapping
    except Exception as e:
        st.error(f"å°ç…§è¡¨è®€å–å¤±æ•—ï¼š{e}")
        return pd.DataFrame(), {}

# --- 3. è™•ç†æ•¸æ“š ---

def process_data(uploaded_file, mapping_dict):
    df = pd.read_csv(uploaded_file, skiprows=1, encoding='utf-8-sig')
    df = df.dropna(subset=['åº', 'é€£çµ¡é›»è©±'])
    df = df[~df['åº'].astype(str).str.contains('å–æ¶ˆ|è½‰ç­|è½‰è®“', na=False)]
    
    extracted_data = df[['å–®ä½', 'å§“å']].copy()
    extracted_data = extracted_data.replace(r'\s+|-|ä¸€åˆ†è™•|ä¸€|ã„§|åˆ†è™•|é€šè¨Š', '', regex=True)
    extracted_data['å–®ä½'] = extracted_data['å–®ä½'].str.upper().apply(lambda x: standardize_unit(x, mapping_dict))

    tocsv_path = 'C:\\Users\\user\\workplace\\RFA\\extracted_data.csv'
    extracted_data.to_csv(tocsv_path, index=False, encoding='utf-8-sig')
    print(f"âœ… æå–çš„è³‡æ–™å·²æˆåŠŸå„²å­˜è‡³ '{tocsv_path}'ã€‚")
    
    return extracted_data

# --- 4. Streamlit ä»‹é¢ ---

st.set_page_config(page_title="RFA å ±åç®¡ç†ç³»çµ±", layout="wide")
st.title("ğŸ“Š RFA å ±åè³‡æ–™å¢é‡æ›´æ–°ç³»çµ± (å®Œæ•´æ¶æ§‹ç‰ˆ)")

MASTER_DB_PATH = 'master_data.csv'
REF_PATH = 'FB11407Fé€šè¨Šè™•20260101.xlsx'

# ç²å–å®Œæ•´æ¸…å–®èˆ‡å­—å…¸
ref_df, mapping_dict = get_full_reference()

# å´é‚Šæ¬„èˆ‡ä¸Šå‚³é‚è¼¯ (èˆ‡å…ˆå‰ç›¸åŒï¼Œç•¥ä½œç²¾ç°¡)
if os.path.exists(MASTER_DB_PATH):
    master_df = pd.read_csv(MASTER_DB_PATH)
    st.sidebar.success(f"ğŸ—ƒï¸ è³‡æ–™åº«ç­†æ•¸: {len(master_df)}")
else:
    master_df = pd.DataFrame(columns=['å–®ä½', 'å§“å'])

uploaded_files = st.file_uploader("ä¸Šå‚³ RFA å ±å CSV", type="csv", accept_multiple_files=True)

if uploaded_files:
    new_dfs = [process_data(f, mapping_dict) for f in uploaded_files]
    current_batch = pd.concat(new_dfs, ignore_index=True)

    st.write("ğŸ” æœ¬æ¬¡ä¸Šå‚³é è¦½ï¼š")
    st.dataframe(current_batch.head(), use_container_width=True)

    if st.button("ğŸš€ ç¢ºèªåˆä½µè‡³ä¸»è³‡æ–™åº«"):
        final_df = pd.concat([master_df, current_batch], ignore_index=True).drop_duplicates(subset=['å–®ä½', 'å§“å'], keep='last')
        final_df.to_csv(MASTER_DB_PATH, index=False, encoding='utf-8-sig')
        st.balloons()
        master_df = final_df

# --- 5. çµ±è¨ˆèˆ‡å ±è¡¨ç”¢å‡º (æ ¸å¿ƒä¿®æ”¹ï¼šLeft Merge) ---

if not master_df.empty and not ref_df.empty:
    st.divider()
    
    # A. ç®—äººæ•¸
    counts = master_df.groupby('å–®ä½').size().reset_index(name='å ±åäººæ•¸')
    
    # B. å°‡äººæ•¸ä½µå›å®Œæ•´æ¸…å–® (ç”¨ã€ŒåŸå§‹æ¸…å–®ã€å»å°ã€Œå–®ä½ã€)
    # é€™æ¨£æ²’å ±åçš„å–®ä½æœƒè®Šæˆ NaNï¼Œæ¨™é¡Œåˆ—ä¹Ÿæœƒä¿ç•™
    final_summary = pd.merge(ref_df, counts, left_on='åŸå§‹æ¸…å–®', right_on='å–®ä½', how='left')
    
    # C. æ¸…ç†çµæœï¼šå°‡å–®ä½çš„ NaN è½‰ç‚º 0ï¼Œä½†ä¿æŒã€Œæ¨™é¡Œåˆ—ã€çš„äººæ•¸ç‚ºç©º(æ¯”è¼ƒç¾è§€)
    final_summary['å ±åäººæ•¸'] = final_summary.apply(
        lambda row: int(row['å ±åäººæ•¸']) if pd.notna(row['å ±åäººæ•¸']) 
        else (0 if row['is_unit'] else ""), axis=1
    )
    
    # åªç•™éœ€è¦çš„æ¬„ä½
    display_summary = final_summary[['åŸå§‹æ¸…å–®', 'å ±åäººæ•¸']]

    st.subheader("ç¬¬äºŒæ­¥ï¼šæ•¸æ“šçµ±è¨ˆ (ä¾é€šè¨ŠéŒ„é †åº)")
    col1, col2 = st.columns([2, 1])
    with col1:
        st.dataframe(display_summary, use_container_width=True, height=600)
    
    with col2:
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            display_summary.to_excel(writer, sheet_name='äººæ•¸çµ±è¨ˆ(ä¾é †åº)', index=False)
            master_df.to_excel(writer, sheet_name='è©³ç´°åå–®', index=False)
        
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´çµ±è¨ˆå ±è¡¨",
            data=buffer.getvalue(),
            file_name=f"RFAå ±åçµ±è¨ˆ_{pd.Timestamp.now().strftime('%m%d')}.xlsx"
        )